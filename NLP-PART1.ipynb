{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a416fbab-e9db-4a30-81ad-193cad0ecbbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in /home/hp/anaconda3/lib/python3.12/site-packages (3.9.1)\n",
      "Requirement already satisfied: click in /home/hp/anaconda3/lib/python3.12/site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in /home/hp/anaconda3/lib/python3.12/site-packages (from nltk) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /home/hp/anaconda3/lib/python3.12/site-packages (from nltk) (2023.10.3)\n",
      "Requirement already satisfied: tqdm in /home/hp/anaconda3/lib/python3.12/site-packages (from nltk) (4.66.4)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install --upgrade nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "23a5f5dc-141e-4c4c-92e3-c6ab8daeca44",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import nltk\n",
    "# nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b77e340-e5af-4d86-a748-94f30837be83",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "df666428-16f8-4ddd-9412-f5744da2b63a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk.corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "81ccd973-c186-4c6d-bcbd-2f5a8d189ba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "AI = '''Artificial Intelligence refers to the intelligence of machines. This is in contrast to the natural intelligence of\n",
    "humans and animals. With Artificial Intelligence, machines perform functions such as learning, planning, reasoning and\n",
    "problem-solving. Most noteworthy, Artificial Intelligence is the simulation of human intelligence by machines.\n",
    "It is probably the fastest-growing development in the World of technology and innovation. Furthermore, many experts believe\n",
    "AI could solve major challenges and crisis situations.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b1d51d7d-b7cc-4660-a26c-305645b48d05",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Artificial Intelligence refers to the intelligence of machines. This is in contrast to the natural intelligence of\\nhumans and animals. With Artificial Intelligence, machines perform functions such as learning, planning, reasoning and\\nproblem-solving. Most noteworthy, Artificial Intelligence is the simulation of human intelligence by machines.\\nIt is probably the fastest-growing development in the World of technology and innovation. Furthermore, many experts believe\\nAI could solve major challenges and crisis situations.'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "AI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0e2fae2c-1c97-4b82-abb0-7f5d1dd06504",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(AI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4af200a2-852f-4175-a0cd-31517d938a6c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Artificial',\n",
       " 'Intelligence',\n",
       " 'refers',\n",
       " 'to',\n",
       " 'the',\n",
       " 'intelligence',\n",
       " 'of',\n",
       " 'machines',\n",
       " '.',\n",
       " 'This',\n",
       " 'is',\n",
       " 'in',\n",
       " 'contrast',\n",
       " 'to',\n",
       " 'the',\n",
       " 'natural',\n",
       " 'intelligence',\n",
       " 'of',\n",
       " 'humans',\n",
       " 'and',\n",
       " 'animals',\n",
       " '.',\n",
       " 'With',\n",
       " 'Artificial',\n",
       " 'Intelligence',\n",
       " ',',\n",
       " 'machines',\n",
       " 'perform',\n",
       " 'functions',\n",
       " 'such',\n",
       " 'as',\n",
       " 'learning',\n",
       " ',',\n",
       " 'planning',\n",
       " ',',\n",
       " 'reasoning',\n",
       " 'and',\n",
       " 'problem-solving',\n",
       " '.',\n",
       " 'Most',\n",
       " 'noteworthy',\n",
       " ',',\n",
       " 'Artificial',\n",
       " 'Intelligence',\n",
       " 'is',\n",
       " 'the',\n",
       " 'simulation',\n",
       " 'of',\n",
       " 'human',\n",
       " 'intelligence',\n",
       " 'by',\n",
       " 'machines',\n",
       " '.',\n",
       " 'It',\n",
       " 'is',\n",
       " 'probably',\n",
       " 'the',\n",
       " 'fastest-growing',\n",
       " 'development',\n",
       " 'in',\n",
       " 'the',\n",
       " 'World',\n",
       " 'of',\n",
       " 'technology',\n",
       " 'and',\n",
       " 'innovation',\n",
       " '.',\n",
       " 'Furthermore',\n",
       " ',',\n",
       " 'many',\n",
       " 'experts',\n",
       " 'believe',\n",
       " 'AI',\n",
       " 'could',\n",
       " 'solve',\n",
       " 'major',\n",
       " 'challenges',\n",
       " 'and',\n",
       " 'crisis',\n",
       " 'situations',\n",
       " '.']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "AI_TOKENS = word_tokenize(AI)\n",
    "AI_TOKENS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "56d9000e-690a-4b44-bb95-cb7406aabf44",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "81"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(AI_TOKENS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f8c8d8f5-fe7e-4403-9a38-bb1b2a0127c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Artificial Intelligence refers to the intelligence of machines.',\n",
       " 'This is in contrast to the natural intelligence of\\nhumans and animals.',\n",
       " 'With Artificial Intelligence, machines perform functions such as learning, planning, reasoning and\\nproblem-solving.',\n",
       " 'Most noteworthy, Artificial Intelligence is the simulation of human intelligence by machines.',\n",
       " 'It is probably the fastest-growing development in the World of technology and innovation.',\n",
       " 'Furthermore, many experts believe\\nAI could solve major challenges and crisis situations.']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "AI_SENT = sent_tokenize(AI)\n",
    "AI_SENT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "30113e03-e97c-4cce-9fc4-3f399a3a3dd7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(AI_SENT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "dc379e5a-f731-4389-9c27-544b3b749379",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Artificial Intelligence refers to the intelligence of machines. This is in contrast to the natural intelligence of\\nhumans and animals. With Artificial Intelligence, machines perform functions such as learning, planning, reasoning and\\nproblem-solving. Most noteworthy, Artificial Intelligence is the simulation of human intelligence by machines.\\nIt is probably the fastest-growing development in the World of technology and innovation. Furthermore, many experts believe\\nAI could solve major challenges and crisis situations.']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import blankline_tokenize\n",
    "BLANK_TOKEN = blankline_tokenize(AI)\n",
    "BLANK_TOKEN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "91c1da78-eccd-4b52-ba49-c0fe64ef7f1a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Artificial',\n",
       " 'Intelligence',\n",
       " 'refers',\n",
       " 'to',\n",
       " 'the',\n",
       " 'intelligence',\n",
       " 'of',\n",
       " 'machines.',\n",
       " 'This',\n",
       " 'is',\n",
       " 'in',\n",
       " 'contrast',\n",
       " 'to',\n",
       " 'the',\n",
       " 'natural',\n",
       " 'intelligence',\n",
       " 'of',\n",
       " 'humans',\n",
       " 'and',\n",
       " 'animals.',\n",
       " 'With',\n",
       " 'Artificial',\n",
       " 'Intelligence,',\n",
       " 'machines',\n",
       " 'perform',\n",
       " 'functions',\n",
       " 'such',\n",
       " 'as',\n",
       " 'learning,',\n",
       " 'planning,',\n",
       " 'reasoning',\n",
       " 'and',\n",
       " 'problem-solving.',\n",
       " 'Most',\n",
       " 'noteworthy,',\n",
       " 'Artificial',\n",
       " 'Intelligence',\n",
       " 'is',\n",
       " 'the',\n",
       " 'simulation',\n",
       " 'of',\n",
       " 'human',\n",
       " 'intelligence',\n",
       " 'by',\n",
       " 'machines.',\n",
       " 'It',\n",
       " 'is',\n",
       " 'probably',\n",
       " 'the',\n",
       " 'fastest-growing',\n",
       " 'development',\n",
       " 'in',\n",
       " 'the',\n",
       " 'World',\n",
       " 'of',\n",
       " 'technology',\n",
       " 'and',\n",
       " 'innovation.',\n",
       " 'Furthermore,',\n",
       " 'many',\n",
       " 'experts',\n",
       " 'believe',\n",
       " 'AI',\n",
       " 'could',\n",
       " 'solve',\n",
       " 'major',\n",
       " 'challenges',\n",
       " 'and',\n",
       " 'crisis',\n",
       " 'situations.']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import WhitespaceTokenizer\n",
    "wt = WhitespaceTokenizer().tokenize(AI)\n",
    "wt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f0dff258-7009-41d0-a240-b585f2d67872",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "70"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(wt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "23791eb6-0f99-49ce-b3da-9648273ffea3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Good Apple costs $4.99 in India. Please buy few of them. Thanks!'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = \"Good Apple costs $4.99 in India. Please buy few of them. Thanks!\"\n",
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "38c15c54-7409-48a1-ad14-5d549c882d93",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Good',\n",
       " 'Apple',\n",
       " 'costs',\n",
       " '$',\n",
       " '4',\n",
       " '.',\n",
       " '99',\n",
       " 'in',\n",
       " 'India',\n",
       " '.',\n",
       " 'Please',\n",
       " 'buy',\n",
       " 'few',\n",
       " 'of',\n",
       " 'them',\n",
       " '.',\n",
       " 'Thanks',\n",
       " '!']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import wordpunct_tokenize\n",
    "wordpunct_tokenize(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c17b9a9d-32f9-4925-a19c-6ff5ded2c4bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Artificial',\n",
       " 'Intelligence',\n",
       " 'refers',\n",
       " 'to',\n",
       " 'the',\n",
       " 'intelligence',\n",
       " 'of',\n",
       " 'machines',\n",
       " '.',\n",
       " 'This',\n",
       " 'is',\n",
       " 'in',\n",
       " 'contrast',\n",
       " 'to',\n",
       " 'the',\n",
       " 'natural',\n",
       " 'intelligence',\n",
       " 'of',\n",
       " 'humans',\n",
       " 'and',\n",
       " 'animals',\n",
       " '.',\n",
       " 'With',\n",
       " 'Artificial',\n",
       " 'Intelligence',\n",
       " ',',\n",
       " 'machines',\n",
       " 'perform',\n",
       " 'functions',\n",
       " 'such',\n",
       " 'as',\n",
       " 'learning',\n",
       " ',',\n",
       " 'planning',\n",
       " ',',\n",
       " 'reasoning',\n",
       " 'and',\n",
       " 'problem',\n",
       " '-',\n",
       " 'solving',\n",
       " '.',\n",
       " 'Most',\n",
       " 'noteworthy',\n",
       " ',',\n",
       " 'Artificial',\n",
       " 'Intelligence',\n",
       " 'is',\n",
       " 'the',\n",
       " 'simulation',\n",
       " 'of',\n",
       " 'human',\n",
       " 'intelligence',\n",
       " 'by',\n",
       " 'machines',\n",
       " '.',\n",
       " 'It',\n",
       " 'is',\n",
       " 'probably',\n",
       " 'the',\n",
       " 'fastest',\n",
       " '-',\n",
       " 'growing',\n",
       " 'development',\n",
       " 'in',\n",
       " 'the',\n",
       " 'World',\n",
       " 'of',\n",
       " 'technology',\n",
       " 'and',\n",
       " 'innovation',\n",
       " '.',\n",
       " 'Furthermore',\n",
       " ',',\n",
       " 'many',\n",
       " 'experts',\n",
       " 'believe',\n",
       " 'AI',\n",
       " 'could',\n",
       " 'solve',\n",
       " 'major',\n",
       " 'challenges',\n",
       " 'and',\n",
       " 'crisis',\n",
       " 'situations',\n",
       " '.']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import wordpunct_tokenize\n",
    "w_p = wordpunct_tokenize(AI)\n",
    "w_p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8d48f1b5-c5cc-45f7-9065-05df5ff96d90",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "85"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(w_p)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05cadd63-bf2c-48d6-bbd3-a19f3e27884d",
   "metadata": {},
   "source": [
    "# NEXT WE WILL SEE HOW WE WILL USE UNI-GRAM,BI-GRAM,TRI-GRAM USING NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "7de8d992-18dd-4577-9bbe-6a8600b4911b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.util import bigrams, trigrams, ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "3a8de91b-fd7d-41ec-abb0-5840cdba1ab7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Developed',\n",
       " 'a',\n",
       " 'facial',\n",
       " 'recognition-based',\n",
       " 'attendance',\n",
       " 'tracking',\n",
       " 'system',\n",
       " ',',\n",
       " 'processing',\n",
       " 'and',\n",
       " 'matching',\n",
       " 'images',\n",
       " 'with',\n",
       " 'Python',\n",
       " 'and',\n",
       " 'integrating',\n",
       " 'with',\n",
       " 'Odoo',\n",
       " 'for',\n",
       " 'automated',\n",
       " 'logging',\n",
       " '.']"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "string = \"Developed a facial recognition-based attendance tracking system, processing and matching images with Python and integrating with Odoo for automated logging.\"\n",
    "quotes_tokens = word_tokenize(string)\n",
    "quotes_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "16ad6178-5d21-432f-ae52-d3be71ed21eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Developed a facial recognition-based attendance tracking system, processing and matching images with Python and integrating with Odoo for automated logging.'"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "931ef427-ef3e-4024-a141-92bb4c492442",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(quotes_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "7b30c182-a711-473d-bbb2-9d100e2c7a91",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Developed', 'a'),\n",
       " ('a', 'facial'),\n",
       " ('facial', 'recognition-based'),\n",
       " ('recognition-based', 'attendance'),\n",
       " ('attendance', 'tracking'),\n",
       " ('tracking', 'system'),\n",
       " ('system', ','),\n",
       " (',', 'processing'),\n",
       " ('processing', 'and'),\n",
       " ('and', 'matching'),\n",
       " ('matching', 'images'),\n",
       " ('images', 'with'),\n",
       " ('with', 'Python'),\n",
       " ('Python', 'and'),\n",
       " ('and', 'integrating'),\n",
       " ('integrating', 'with'),\n",
       " ('with', 'Odoo'),\n",
       " ('Odoo', 'for'),\n",
       " ('for', 'automated'),\n",
       " ('automated', 'logging'),\n",
       " ('logging', '.')]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quotes_bigrams = list(nltk.bigrams(quotes_tokens))\n",
    "quotes_bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "90b2dad7-b467-4861-a0e6-9ce68e70cea6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Developed', 'a', 'facial'),\n",
       " ('a', 'facial', 'recognition-based'),\n",
       " ('facial', 'recognition-based', 'attendance'),\n",
       " ('recognition-based', 'attendance', 'tracking'),\n",
       " ('attendance', 'tracking', 'system'),\n",
       " ('tracking', 'system', ','),\n",
       " ('system', ',', 'processing'),\n",
       " (',', 'processing', 'and'),\n",
       " ('processing', 'and', 'matching'),\n",
       " ('and', 'matching', 'images'),\n",
       " ('matching', 'images', 'with'),\n",
       " ('images', 'with', 'Python'),\n",
       " ('with', 'Python', 'and'),\n",
       " ('Python', 'and', 'integrating'),\n",
       " ('and', 'integrating', 'with'),\n",
       " ('integrating', 'with', 'Odoo'),\n",
       " ('with', 'Odoo', 'for'),\n",
       " ('Odoo', 'for', 'automated'),\n",
       " ('for', 'automated', 'logging'),\n",
       " ('automated', 'logging', '.')]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quotes_bigrams = list(nltk.trigrams(quotes_tokens))\n",
    "quotes_bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "2030bae9-342b-4c4c-b945-ae2ba3d349f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Developed', 'a', 'facial', 'recognition-based', 'attendance'),\n",
       " ('a', 'facial', 'recognition-based', 'attendance', 'tracking'),\n",
       " ('facial', 'recognition-based', 'attendance', 'tracking', 'system'),\n",
       " ('recognition-based', 'attendance', 'tracking', 'system', ','),\n",
       " ('attendance', 'tracking', 'system', ',', 'processing'),\n",
       " ('tracking', 'system', ',', 'processing', 'and'),\n",
       " ('system', ',', 'processing', 'and', 'matching'),\n",
       " (',', 'processing', 'and', 'matching', 'images'),\n",
       " ('processing', 'and', 'matching', 'images', 'with'),\n",
       " ('and', 'matching', 'images', 'with', 'Python'),\n",
       " ('matching', 'images', 'with', 'Python', 'and'),\n",
       " ('images', 'with', 'Python', 'and', 'integrating'),\n",
       " ('with', 'Python', 'and', 'integrating', 'with'),\n",
       " ('Python', 'and', 'integrating', 'with', 'Odoo'),\n",
       " ('and', 'integrating', 'with', 'Odoo', 'for'),\n",
       " ('integrating', 'with', 'Odoo', 'for', 'automated'),\n",
       " ('with', 'Odoo', 'for', 'automated', 'logging'),\n",
       " ('Odoo', 'for', 'automated', 'logging', '.')]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quotes_bigrams = list(nltk.ngrams(quotes_tokens,5))\n",
    "quotes_bigrams"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e89e3de5-f836-444e-ac64-c0615888229d",
   "metadata": {},
   "source": [
    "<!-- Normalizing words into their base/root forms using stemming/lemmatization -->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0abb5937-382b-499f-a891-c42a5b91dd2c",
   "metadata": {},
   "source": [
    "# Normalizing words into their base/roots forms using stemming/lemmatization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01970e31-e7d4-4856-a242-5f8abeb9ca1e",
   "metadata": {},
   "source": [
    "# Porter Stemmer --> root form of the word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "51c88d58-46e9-4570-8f98-cfe850dd5611",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "pst = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "a088b422-808d-409e-b5da-55a335fc8700",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'affect'"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pst.stem('affection')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "78f7d1c5-e27f-42e0-9661-ab6d4ffe0753",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'player'"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pst.stem('player')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "bad49f96-cfb6-4858-b2ef-aac086e39660",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'play'"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pst.stem('playing')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "8432f30c-2a1c-4217-bb5e-73011ab81bb6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'histor'"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pst.stem('historical')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "4eff7d17-d3e3-433b-b8d4-3a4c07daa574",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'histori'"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pst.stem('history')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "53f19369-e915-4075-95a7-55cb2c6c3a55",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'maximum'"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pst.stem('maximum')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "94800752-d146-42d5-8d6b-36b8ff9e9bce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word: give\n",
      "word: gave\n",
      "word: give\n",
      "word: given\n"
     ]
    }
   ],
   "source": [
    "words_to_stem = ['give', 'gave', 'giving', 'given']\n",
    "\n",
    "for word in words_to_stem:\n",
    "    print('word'+ ':' , pst.stem(word))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "f81a1d3d-79c1-40fb-b3a9-3853e34ce144",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word: give\n",
      "word: gave\n",
      "word: give\n",
      "word: gave\n",
      "word: thanksgiv\n",
      "word: thank\n",
      "word: think\n",
      "word: love\n",
      "word: maximum\n",
      "word: go\n",
      "word: go\n",
      "word: gone\n"
     ]
    }
   ],
   "source": [
    "words_to_stem = ['give', 'gave', 'giving', 'gaved', 'thanksgiving', 'thanks', 'thinking' , 'loving', 'maximum','go','going','gone']\n",
    "\n",
    "for word in words_to_stem:\n",
    "    print('word'+ ':' , pst.stem(word))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8b673c6-447b-4ef0-b773-b25ffd6ee9c6",
   "metadata": {},
   "source": [
    "# LancasterStemmer --> core root form of the word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "c29931a8-e279-4547-9f68-145e049b246d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word: giv\n",
      "word: gav\n",
      "word: giv\n",
      "word: gav\n",
      "word: thanksg\n",
      "word: thank\n",
      "word: think\n",
      "word: lov\n",
      "word: maxim\n",
      "word: go\n",
      "word: going\n",
      "word: gon\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import LancasterStemmer\n",
    "lst = LancasterStemmer()\n",
    "\n",
    "for word in words_to_stem:\n",
    "    print('word'+ ':' , lst.stem(word))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0042615-2e37-4e58-be22-32e97e543a34",
   "metadata": {},
   "source": [
    "# SnowballStemmer --> core root form of the word as per language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "69058be5-c67e-494a-94bf-919406abe695",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word: give\n",
      "word: gave\n",
      "word: give\n",
      "word: gave\n",
      "word: thanksgiv\n",
      "word: thank\n",
      "word: think\n",
      "word: love\n",
      "word: maximum\n",
      "word: go\n",
      "word: go\n",
      "word: gone\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import SnowballStemmer\n",
    "sst = SnowballStemmer('english')\n",
    "\n",
    "for word in words_to_stem:\n",
    "    print('word'+ ':' , sst.stem(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "a27a71ea-64c2-495b-8e7d-bf7bfe08073f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word: giv\n",
      "word: gav\n",
      "word: giving\n",
      "word: gaved\n",
      "word: thanksgiving\n",
      "word: thank\n",
      "word: thinking\n",
      "word: loving\n",
      "word: maximum\n",
      "word: go\n",
      "word: going\n",
      "word: gon\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import SnowballStemmer\n",
    "sst = SnowballStemmer('french')\n",
    "\n",
    "for word in words_to_stem:\n",
    "    print('word'+ ':' , sst.stem(word))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7425151-de13-4906-8386-f3b4f3559b3d",
   "metadata": {},
   "source": [
    "# Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "337fe9f1-f260-4a36-a7d9-e60c9ecabafd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "word_net = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "32b98f2a-56c5-48eb-a21b-0300beef5c55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word: give\n",
      "word: gave\n",
      "word: giving\n",
      "word: gaved\n",
      "word: thanksgiving\n",
      "word: thanks\n",
      "word: thinking\n",
      "word: loving\n",
      "word: maximum\n",
      "word: go\n",
      "word: going\n",
      "word: gone\n"
     ]
    }
   ],
   "source": [
    "words_to_stem = ['give', 'gave', 'giving', 'gaved', 'thanksgiving', 'thanks', 'thinking' , 'loving', 'maximum','go','going','gone']\n",
    "\n",
    "for word in words_to_stem:\n",
    "    print('word'+ ':' , word_net.lemmatize(word))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "7693c666-9976-4c3e-976a-d2f14fc374dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# StopWords -->  in any language the repeating words are called stopwords e.g.- and, for,i etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "f14a0a60-4855-4626-a2c3-c188f2abf1b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "7b29c4fd-6c5c-475f-b5bf-7a92c2032e7c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i',\n",
       " 'me',\n",
       " 'my',\n",
       " 'myself',\n",
       " 'we',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'you',\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " \"you'll\",\n",
       " \"you'd\",\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves',\n",
       " 'he',\n",
       " 'him',\n",
       " 'his',\n",
       " 'himself',\n",
       " 'she',\n",
       " \"she's\",\n",
       " 'her',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'it',\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " 'they',\n",
       " 'them',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'themselves',\n",
       " 'what',\n",
       " 'which',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'this',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'these',\n",
       " 'those',\n",
       " 'am',\n",
       " 'is',\n",
       " 'are',\n",
       " 'was',\n",
       " 'were',\n",
       " 'be',\n",
       " 'been',\n",
       " 'being',\n",
       " 'have',\n",
       " 'has',\n",
       " 'had',\n",
       " 'having',\n",
       " 'do',\n",
       " 'does',\n",
       " 'did',\n",
       " 'doing',\n",
       " 'a',\n",
       " 'an',\n",
       " 'the',\n",
       " 'and',\n",
       " 'but',\n",
       " 'if',\n",
       " 'or',\n",
       " 'because',\n",
       " 'as',\n",
       " 'until',\n",
       " 'while',\n",
       " 'of',\n",
       " 'at',\n",
       " 'by',\n",
       " 'for',\n",
       " 'with',\n",
       " 'about',\n",
       " 'against',\n",
       " 'between',\n",
       " 'into',\n",
       " 'through',\n",
       " 'during',\n",
       " 'before',\n",
       " 'after',\n",
       " 'above',\n",
       " 'below',\n",
       " 'to',\n",
       " 'from',\n",
       " 'up',\n",
       " 'down',\n",
       " 'in',\n",
       " 'out',\n",
       " 'on',\n",
       " 'off',\n",
       " 'over',\n",
       " 'under',\n",
       " 'again',\n",
       " 'further',\n",
       " 'then',\n",
       " 'once',\n",
       " 'here',\n",
       " 'there',\n",
       " 'when',\n",
       " 'where',\n",
       " 'why',\n",
       " 'how',\n",
       " 'all',\n",
       " 'any',\n",
       " 'both',\n",
       " 'each',\n",
       " 'few',\n",
       " 'more',\n",
       " 'most',\n",
       " 'other',\n",
       " 'some',\n",
       " 'such',\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'only',\n",
       " 'own',\n",
       " 'same',\n",
       " 'so',\n",
       " 'than',\n",
       " 'too',\n",
       " 'very',\n",
       " 's',\n",
       " 't',\n",
       " 'can',\n",
       " 'will',\n",
       " 'just',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'should',\n",
       " \"should've\",\n",
       " 'now',\n",
       " 'd',\n",
       " 'll',\n",
       " 'm',\n",
       " 'o',\n",
       " 're',\n",
       " 've',\n",
       " 'y',\n",
       " 'ain',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'ma',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\"]"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "179df54b-7ed7-4088-bb2a-b88229300b16",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "179"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "ebd25c59-5401-47eb-b4cc-fafa4f4eb04e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['au',\n",
       " 'aux',\n",
       " 'avec',\n",
       " 'ce',\n",
       " 'ces',\n",
       " 'dans',\n",
       " 'de',\n",
       " 'des',\n",
       " 'du',\n",
       " 'elle',\n",
       " 'en',\n",
       " 'et',\n",
       " 'eux',\n",
       " 'il',\n",
       " 'ils',\n",
       " 'je',\n",
       " 'la',\n",
       " 'le',\n",
       " 'les',\n",
       " 'leur',\n",
       " 'lui',\n",
       " 'ma',\n",
       " 'mais',\n",
       " 'me',\n",
       " 'même',\n",
       " 'mes',\n",
       " 'moi',\n",
       " 'mon',\n",
       " 'ne',\n",
       " 'nos',\n",
       " 'notre',\n",
       " 'nous',\n",
       " 'on',\n",
       " 'ou',\n",
       " 'par',\n",
       " 'pas',\n",
       " 'pour',\n",
       " 'qu',\n",
       " 'que',\n",
       " 'qui',\n",
       " 'sa',\n",
       " 'se',\n",
       " 'ses',\n",
       " 'son',\n",
       " 'sur',\n",
       " 'ta',\n",
       " 'te',\n",
       " 'tes',\n",
       " 'toi',\n",
       " 'ton',\n",
       " 'tu',\n",
       " 'un',\n",
       " 'une',\n",
       " 'vos',\n",
       " 'votre',\n",
       " 'vous',\n",
       " 'c',\n",
       " 'd',\n",
       " 'j',\n",
       " 'l',\n",
       " 'à',\n",
       " 'm',\n",
       " 'n',\n",
       " 's',\n",
       " 't',\n",
       " 'y',\n",
       " 'été',\n",
       " 'étée',\n",
       " 'étées',\n",
       " 'étés',\n",
       " 'étant',\n",
       " 'étante',\n",
       " 'étants',\n",
       " 'étantes',\n",
       " 'suis',\n",
       " 'es',\n",
       " 'est',\n",
       " 'sommes',\n",
       " 'êtes',\n",
       " 'sont',\n",
       " 'serai',\n",
       " 'seras',\n",
       " 'sera',\n",
       " 'serons',\n",
       " 'serez',\n",
       " 'seront',\n",
       " 'serais',\n",
       " 'serait',\n",
       " 'serions',\n",
       " 'seriez',\n",
       " 'seraient',\n",
       " 'étais',\n",
       " 'était',\n",
       " 'étions',\n",
       " 'étiez',\n",
       " 'étaient',\n",
       " 'fus',\n",
       " 'fut',\n",
       " 'fûmes',\n",
       " 'fûtes',\n",
       " 'furent',\n",
       " 'sois',\n",
       " 'soit',\n",
       " 'soyons',\n",
       " 'soyez',\n",
       " 'soient',\n",
       " 'fusse',\n",
       " 'fusses',\n",
       " 'fût',\n",
       " 'fussions',\n",
       " 'fussiez',\n",
       " 'fussent',\n",
       " 'ayant',\n",
       " 'ayante',\n",
       " 'ayantes',\n",
       " 'ayants',\n",
       " 'eu',\n",
       " 'eue',\n",
       " 'eues',\n",
       " 'eus',\n",
       " 'ai',\n",
       " 'as',\n",
       " 'avons',\n",
       " 'avez',\n",
       " 'ont',\n",
       " 'aurai',\n",
       " 'auras',\n",
       " 'aura',\n",
       " 'aurons',\n",
       " 'aurez',\n",
       " 'auront',\n",
       " 'aurais',\n",
       " 'aurait',\n",
       " 'aurions',\n",
       " 'auriez',\n",
       " 'auraient',\n",
       " 'avais',\n",
       " 'avait',\n",
       " 'avions',\n",
       " 'aviez',\n",
       " 'avaient',\n",
       " 'eut',\n",
       " 'eûmes',\n",
       " 'eûtes',\n",
       " 'eurent',\n",
       " 'aie',\n",
       " 'aies',\n",
       " 'ait',\n",
       " 'ayons',\n",
       " 'ayez',\n",
       " 'aient',\n",
       " 'eusse',\n",
       " 'eusses',\n",
       " 'eût',\n",
       " 'eussions',\n",
       " 'eussiez',\n",
       " 'eussent']"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopwords.words('french')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "bc48183c-bf1b-4ab5-83db-46193701b8d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['de',\n",
       " 'en',\n",
       " 'van',\n",
       " 'ik',\n",
       " 'te',\n",
       " 'dat',\n",
       " 'die',\n",
       " 'in',\n",
       " 'een',\n",
       " 'hij',\n",
       " 'het',\n",
       " 'niet',\n",
       " 'zijn',\n",
       " 'is',\n",
       " 'was',\n",
       " 'op',\n",
       " 'aan',\n",
       " 'met',\n",
       " 'als',\n",
       " 'voor',\n",
       " 'had',\n",
       " 'er',\n",
       " 'maar',\n",
       " 'om',\n",
       " 'hem',\n",
       " 'dan',\n",
       " 'zou',\n",
       " 'of',\n",
       " 'wat',\n",
       " 'mijn',\n",
       " 'men',\n",
       " 'dit',\n",
       " 'zo',\n",
       " 'door',\n",
       " 'over',\n",
       " 'ze',\n",
       " 'zich',\n",
       " 'bij',\n",
       " 'ook',\n",
       " 'tot',\n",
       " 'je',\n",
       " 'mij',\n",
       " 'uit',\n",
       " 'der',\n",
       " 'daar',\n",
       " 'haar',\n",
       " 'naar',\n",
       " 'heb',\n",
       " 'hoe',\n",
       " 'heeft',\n",
       " 'hebben',\n",
       " 'deze',\n",
       " 'u',\n",
       " 'want',\n",
       " 'nog',\n",
       " 'zal',\n",
       " 'me',\n",
       " 'zij',\n",
       " 'nu',\n",
       " 'ge',\n",
       " 'geen',\n",
       " 'omdat',\n",
       " 'iets',\n",
       " 'worden',\n",
       " 'toch',\n",
       " 'al',\n",
       " 'waren',\n",
       " 'veel',\n",
       " 'meer',\n",
       " 'doen',\n",
       " 'toen',\n",
       " 'moet',\n",
       " 'ben',\n",
       " 'zonder',\n",
       " 'kan',\n",
       " 'hun',\n",
       " 'dus',\n",
       " 'alles',\n",
       " 'onder',\n",
       " 'ja',\n",
       " 'eens',\n",
       " 'hier',\n",
       " 'wie',\n",
       " 'werd',\n",
       " 'altijd',\n",
       " 'doch',\n",
       " 'wordt',\n",
       " 'wezen',\n",
       " 'kunnen',\n",
       " 'ons',\n",
       " 'zelf',\n",
       " 'tegen',\n",
       " 'na',\n",
       " 'reeds',\n",
       " 'wil',\n",
       " 'kon',\n",
       " 'niets',\n",
       " 'uw',\n",
       " 'iemand',\n",
       " 'geweest',\n",
       " 'andere']"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopwords.words('dutch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "322bb99e-1d62-40cf-9388-3fa0e20a7b90",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "157"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(stopwords.words('french'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "527e2076-e83d-403d-bf01-9aadacc5daf6",
   "metadata": {},
   "source": [
    "# Parts of Speech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "08b35797-d68e-47e4-96c8-5beee3891e21",
   "metadata": {},
   "outputs": [],
   "source": [
    "#POS [part of sppech] is always talking about grammaticaly type of the word called verbs, noun, adjective, proverb,\n",
    "#how the word will function in grammatically within the sentence, a word can have more then one pos based on context in which it will use\n",
    "#so lets see some pos tags & description, so pos tags are usualy used to descrie weather te word is used for noun,adjective,pronoun, propernoun, singular, plural, is it symbol or is it adverb\n",
    "#in this slide we have so many tags along with their description with different tags\n",
    "#this tags are beginning from coordinating conjunction to whadverb & lets understand about one of the example \n",
    "#next we will see how we will implement this POS in our text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "30333c43-d105-4941-a51c-c735e7c06ef2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Artificial Intelligence refers to the intelligence of machines. This is in contrast to the natural intelligence of\\nhumans and animals. With Artificial Intelligence, machines perform functions such as learning, planning, reasoning and\\nproblem-solving. Most noteworthy, Artificial Intelligence is the simulation of human intelligence by machines.\\nIt is probably the fastest-growing development in the World of technology and innovation. Furthermore, many experts believe\\nAI could solve major challenges and crisis situations.'"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "AI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "c16576f7-55a5-475d-8163-35bc7bd21e88",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Sam', 'is', 'natural', 'when', 'it', 'comes', 'to', 'drawing']"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent = \"Sam is natural when it comes to drawing\"\n",
    "sent_tokens = word_tokenize(sent)\n",
    "sent_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "be988815-e68b-49cc-8a3e-350ea2b028e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Sam', 'NNP')]\n",
      "[('is', 'VBZ')]\n",
      "[('natural', 'JJ')]\n",
      "[('when', 'WRB')]\n",
      "[('it', 'PRP')]\n",
      "[('comes', 'VBZ')]\n",
      "[('to', 'TO')]\n",
      "[('drawing', 'VBG')]\n"
     ]
    }
   ],
   "source": [
    "for token in sent_tokens:\n",
    "    print(nltk.pos_tag([token]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "af2bc59f-7bfd-43ca-b9c1-a9923c8404ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['john', 'is', 'eating', 'delicious', 'cake']"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent2 = \"john is eating delicious cake\"\n",
    "sent2_tokens = word_tokenize(sent2)\n",
    "sent2_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "f7dd3331-0768-48c3-8cfe-de9c8e9b157b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('john', 'NN')]\n",
      "[('is', 'VBZ')]\n",
      "[('eating', 'VBG')]\n",
      "[('delicious', 'JJ')]\n",
      "[('cake', 'NN')]\n"
     ]
    }
   ],
   "source": [
    "for token in sent2_tokens:\n",
    "    print(nltk.pos_tag([token]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72d91233-0871-435a-9dd0-f718904cc8d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Another concept of POS is called NER ( NAMED ENTITIY RECOGNITION ), NER is the process of detecting name such as movie, moneytary value,organiztion, location, quantities & person\n",
    "# there are 3 phases of NER - ( 1ST PHASE IS - NOUN PHRASE EXTRACTION OR NOUN PHASE IDENTIFICATION - This step deals with extract all the noun phrases from text using dependencies parsing and pos tagging\n",
    "# 2nd step we have phrase classification - this is the classification where all the extracted nouns & phrase are classified into category such as location,names and much more \n",
    "# some times entity are misclassification \n",
    "# so if you are use NER in python then you need to import NER_CHUNK from nltk library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "c2f2f712-6e64-4ba8-a58a-1ce39573d404",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import ne_chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "a869a462-e759-4def-a4e0-dbf87542778a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The', 'US', 'President', 'stays', 'in', 'the', 'whitehouse']"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NE_sent = \" The US President stays in the whitehouse\"\n",
    "NE_sent_tokens = word_tokenize(NE_sent)\n",
    "NE_sent_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "b3dd39b6-e7fa-43e2-9865-ed23a018a73b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('The', 'DT'),\n",
       " ('US', 'NNP'),\n",
       " ('President', 'NNP'),\n",
       " ('stays', 'VBZ'),\n",
       " ('in', 'IN'),\n",
       " ('the', 'DT'),\n",
       " ('whitehouse', 'NN')]"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NE_TAGS = nltk.pos_tag(NE_sent_tokens)\n",
    "NE_TAGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "133859bb-fc13-41d3-9b2a-477fc74f2d5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# IN NLTK also we have syntax- set of rules,principals & process \n",
    "# lets understand set of rules & that will indicates the syntax tree & in the real time also you have build this type of tree from the sentenses\n",
    "# now lets understand the important concept called CHUNKING using the sentence structure\n",
    "# chunking means grouping of words into chunks & lets understand the example of chunking \n",
    "# chunking will help to easy process the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "d634d5cb-8020-4cf4-a788-46d99579955a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  The/DT\n",
      "  (ORGANIZATION US/NNP)\n",
      "  President/NNP\n",
      "  stays/VBZ\n",
      "  in/IN\n",
      "  the/DT\n",
      "  whitehouse/NN)\n"
     ]
    }
   ],
   "source": [
    "NE_NER = ne_chunk(NE_TAGS)\n",
    "print(NE_NER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c873a7f-658d-4f08-966c-9c562ae86223",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
